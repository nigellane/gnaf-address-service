# Story 2.1: G-NAF Dataset Integration and Infrastructure

## Status
Approved

## Story
**As a** system administrator,  
**I want** automated G-NAF dataset processing and management infrastructure,  
**so that** the service maintains current, accurate Australian address data with minimal manual intervention.

## Acceptance Criteria

1. G-NAF dataset (~13GB) imported with full PostGIS spatial indexing
2. Automated quarterly update pipeline with rollback capabilities  
3. Database schema optimized for address search performance
4. Data quality validation and integrity checking
5. Comprehensive monitoring and alerting for dataset freshness

## Tasks / Subtasks

### Database Infrastructure Setup
- [x] Set up PostgreSQL database with PostGIS extension (AC: 1, 3)
- [x] Create database schema for G-NAF tables with spatial indexing (AC: 1, 3)
- [x] Implement connection pooling and performance optimization (AC: 3)
- [x] Create database migration scripts for schema versioning (AC: 3)

### G-NAF Dataset Import Pipeline  
- [x] Download G-NAF dataset from data.gov.au (AC: 1)
- [ ] Parse G-NAF CSV files and transform to database schema (AC: 1)
- [ ] Implement bulk data import with progress monitoring (AC: 1)
- [ ] Create spatial indexes on coordinate columns (AC: 1, 3)
- [ ] Generate address search indexes for full-text search (AC: 3)

### Data Quality and Validation
- [ ] Implement data integrity checks during import (AC: 4)
- [ ] Create address completeness validation rules (AC: 4)  
- [ ] Validate coordinate precision and boundaries (AC: 4)
- [ ] Generate data quality reports and statistics (AC: 4)

### Automated Update Pipeline
- [ ] Create scheduled job for quarterly G-NAF dataset checks (AC: 2)
- [ ] Implement incremental update detection and processing (AC: 2)
- [ ] Create rollback mechanism for failed updates (AC: 2)
- [ ] Add pipeline monitoring and error notifications (AC: 2, 5)

### Monitoring and Alerting
- [ ] Implement dataset freshness monitoring (AC: 5)
- [ ] Create health checks for database connectivity (AC: 5)
- [ ] Add performance metrics for query response times (AC: 5)
- [ ] Set up alerting for import failures and data staleness (AC: 5)

### Testing
- [ ] Unit tests for data import functions
- [ ] Integration tests for database schema and indexes
- [ ] Performance tests for large dataset imports
- [ ] End-to-end tests for update pipeline

## Dev Notes

### Previous Story Insights
No previous stories - this is the foundational infrastructure story.

### Data Models
Based on existing TypeScript definitions in `src/types/address.ts`:

- **GNAFAddress Interface**: Complete address structure with components, coordinates, quality metrics, boundaries, and metadata [Source: src/types/address.ts]
- **AddressComponents**: Structured address parsing including building, flat, street, locality, state, postcode [Source: src/types/address.ts]  
- **Coordinates**: Longitude/latitude with precision indicators and CRS support [Source: src/types/address.ts]
- **AddressQuality**: Confidence scoring, reliability codes, completeness metrics [Source: src/types/address.ts]
- **AdministrativeBoundaries**: LGA, electoral, and statistical area mappings [Source: src/types/address.ts]

### Database Schema Requirements
Based on README.md architecture specifications:

- **PostgreSQL 15+** with PostGIS extension for spatial operations [Source: README.md#Prerequisites]
- **Spatial Indexing**: Required for coordinate-based queries and proximity searches [Source: README.md#Architecture]
- **Full-text Search**: Address text search capabilities with GIN indexes [Source: README.md#Key Features]
- **Performance Target**: Support for <300ms address validation response times [Source: README.md#Performance Targets]

### API Specifications
No specific database API requirements found in architecture docs. Implementation should follow RESTful patterns established in the codebase.

### Component Specifications  
No UI components required for this infrastructure story.

### File Locations
Based on existing project structure:

- **Database Scripts**: `/scripts/setup-database.js`, `/scripts/migrate-database.js` [Source: package.json scripts]
- **Import Scripts**: `/scripts/download-gnaf.js`, `/scripts/import-gnaf.js` [Source: package.json scripts]
- **Configuration**: Database connection via `DATABASE_URL` environment variable [Source: README.md#Environment Variables]
- **Data Storage**: G-NAF dataset path via `GNAF_DATASET_PATH` environment variable [Source: README.md#Environment Variables]

### Testing Requirements
Based on package.json configuration:

- **Test Framework**: Jest with TypeScript support via ts-jest [Source: package.json devDependencies]
- **Coverage Target**: Use `npm run test:coverage` for coverage reporting [Source: package.json scripts]
- **Integration Testing**: Supertest available for API endpoint testing [Source: package.json devDependencies]

### Technical Constraints
Based on project configuration:

- **Node.js Version**: >= 20.0.0 [Source: package.json engines]
- **TypeScript**: Required for all source code [Source: package.json devDependencies]
- **Database**: PostgreSQL with connection pooling via `pg` library [Source: package.json dependencies]
- **CSV Processing**: Use `csv-parse` and `stream-transform` for large file handling [Source: package.json dependencies]
- **Logging**: Winston for structured logging [Source: package.json dependencies]

### Performance Considerations
- Large dataset (~13GB) requires streaming and batch processing
- Spatial indexing critical for coordinate-based queries
- Connection pooling required for concurrent access
- Progress monitoring for long-running import operations

### Project Structure Notes
The existing project structure aligns well with the database infrastructure requirements. Scripts directory is already established for database operations, and environment variable patterns are consistent with the architecture.

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-01-09 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
Claude Sonnet 4 (claude-sonnet-4-20250514)

### Debug Log References  
- Database Infrastructure Setup: Completed all 4 subtasks
- G-NAF Dataset Import Pipeline: 1 of 5 subtasks completed
- In progress: Parsing G-NAF CSV files

### Completion Notes
**Completed Tasks:**
- Database setup script with PostGIS extensions
- Comprehensive database schema with spatial indexing and triggers
- Connection pooling with performance optimization
- Migration system with version control and rollback capability
- G-NAF dataset download script with progress monitoring

**Current Task:** Implementing CSV parser for G-NAF data transformation

### File List
**Created/Modified Files:**
- scripts/setup-database.js - PostgreSQL with PostGIS setup
- scripts/create-schema.sql - Complete database schema with spatial indexing
- scripts/create-schema.js - Schema execution script  
- scripts/migrate-database.js - Migration management system
- scripts/migrations/001_initial_schema.sql - Initial migration
- scripts/migrations/rollbacks/001_initial_schema.sql - Rollback script
- src/config/database.ts - Database connection pooling and optimization
- scripts/download-gnaf.js - G-NAF dataset download automation
- package.json - Added pg-copy-streams dependency

## QA Results
_To be filled by QA Agent_